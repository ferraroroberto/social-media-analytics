# HOW-TO: Learn Data Science Through Content Performance Prediction

## üéØ Introduction

This document is your comprehensive guide to learning data science by building and understanding the Content Performance Predictor project. Unlike traditional tutorials that just show you how to run code, this guide will teach you **why** each piece exists, **how** it works, and **what** it accomplishes.

**Who this is for:** You have technical knowledge and programming experience, but you're new to machine learning and data science. You want to understand the complete pipeline, not just execute it.

**What you'll learn:** The entire data science workflow from data collection to model deployment, with hands-on examples for each component.

---

## üìö Prerequisites & Setup

### What You Need
- Python 3.8+ installed
- Basic understanding of Python (functions, classes, imports)
- Familiarity with command line
- A Supabase account (we'll set this up)

### Project Setup
```bash
# Clone and setup
git clone <repository-url>
cd content-performance-predictor
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

---

## üóÇÔ∏è Understanding the Project Structure

Before diving into code, let's understand **why** this project is organized this way:

```
src/
‚îú‚îÄ‚îÄ data/           # Data access layer - connects to databases
‚îú‚îÄ‚îÄ features/       # Feature engineering - transforms raw data
‚îú‚îÄ‚îÄ models/         # Machine learning models - learns patterns
‚îú‚îÄ‚îÄ api/            # Web interface - makes predictions available
‚îî‚îÄ‚îÄ utils/          # Configuration and helpers
```

**Why this structure?** Data science projects follow a pipeline: Data ‚Üí Features ‚Üí Models ‚Üí Predictions. Each folder represents one stage.

---

## üîå Step 1: Data Layer - Understanding Data Sources

### What We're Learning
- How to connect to databases
- How to structure data queries
- How to handle different data formats

### The Database Connection (`src/data/database.py`)

**What it does:** Creates a connection to Supabase (PostgreSQL database) and provides methods to fetch data.

**Key Concepts:**
- **Client Pattern:** One class handles all database operations
- **Environment Variables:** Sensitive data (URLs, keys) stored outside code
- **Error Handling:** Graceful failure when database is unavailable

**Let's explore it step by step:**

```python
# Open src/data/database.py and look at the SupabaseClient class
class SupabaseClient:
    def __init__(self):
        # Gets credentials from environment variables
        self.url = os.getenv("SUPABASE_URL")
        self.key = os.getenv("SUPABASE_KEY")
        
        # Creates connection
        self.client: Client = create_client(self.url, self.key)
```

**Why environment variables?** 
- Security: Credentials never in code
- Flexibility: Different settings for development/production
- Team collaboration: Each developer can have their own database

**Try it yourself:**
```bash
# Create .env file
cp .env.example .env
# Edit .env with your Supabase credentials
```

**What the data looks like:**
The `posts` table has columns like:
- `num_likes_linkedin_no_video` - engagement metrics
- `date` - when the post was made
- Platform-specific columns for different social networks

**Why this structure?** Social media data is naturally time-series (daily posts) with multiple platforms and content types.

---

## üîß Step 2: Feature Engineering - Transforming Raw Data

### What We're Learning
- How to create features that help models learn
- Time-series feature engineering
- Data preprocessing techniques

### The Feature Engineer (`src/features/feature_engineering.py`)

**What it does:** Transforms raw data into features that machine learning models can understand.

**Key Concepts:**
- **Temporal Features:** Day of week, month, seasonality
- **Lag Features:** Previous values to capture trends
- **Rolling Statistics:** Moving averages, standard deviations
- **Cyclical Encoding:** Converting circular data (like days) to continuous values

**Let's break down temporal features:**

```python
def create_temporal_features(self, df: pd.DataFrame, date_col: str = 'date') -> pd.DataFrame:
    # Convert string dates to datetime objects
    df[date_col] = pd.to_datetime(df[date_col])
    
    # Extract components
    df['day_of_week'] = df[date_col].dt.dayofweek  # 0=Monday, 6=Sunday
    df['month'] = df[date_col].dt.month             # 1-12
    df['quarter'] = df[date_col].dt.quarter         # 1-4
    
    # Cyclical encoding (why? because month 12 is close to month 1)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
```

**Why cyclical encoding?** 
- Month 12 (December) is close to Month 1 (January)
- Regular encoding (1, 2, 3... 12) makes December far from January
- Sin/cos encoding creates a circle where December and January are close

**Try it yourself:**
```python
# In Python console
import numpy as np
import pandas as pd

dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
df = pd.DataFrame({'date': dates})
df['month'] = df['date'].dt.month
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

# Plot to see the cyclical pattern
import matplotlib.pyplot as plt
plt.scatter(df['month_sin'], df['month_cos'])
plt.show()
```

**Lag features explained:**
```python
def create_lag_features(self, df: pd.DataFrame, target_cols: List[str], lags: List[int] = [1, 3, 7, 14, 30]):
    for col in target_cols:
        for lag in lags:
            # Yesterday's value
            df[f'{col}_lag_{lag}'] = df[col].shift(lag)
            
            # Rolling average over last N days
            df[f'{col}_rolling_mean_{lag}'] = df[col].rolling(window=lag, min_periods=1).mean()
```

**Why lag features?** 
- Yesterday's engagement predicts today's engagement
- Rolling averages smooth out daily fluctuations
- Models learn patterns over time, not just current values

---

## ü§ñ Step 3: Machine Learning Models - Learning from Data

### What We're Learning
- How different algorithms work
- Model training and evaluation
- Hyperparameter tuning
- Model persistence

### The Base Model (`src/models/base_model.py`)

**What it does:** Provides a common interface for all machine learning models, ensuring consistency in training, prediction, and evaluation.

**Key Concepts:**
- **Abstract Base Class:** Defines what all models must do
- **Inheritance:** Specific models (Random Forest, XGBoost) inherit common functionality
- **MLflow Integration:** Tracks experiments and model versions
- **Feature Importance:** Understands which features matter most

**The training process:**
```python
def fit(self, X: pd.DataFrame, y: pd.Series) -> 'BaseModel':
    # X = features (day of week, month, lag features, etc.)
    # y = target (number of likes, comments, etc.)
    
    # Clean data (remove rows with missing values)
    mask = ~(X.isnull().any(axis=1) | y.isnull())
    X_clean = X[mask]
    y_clean = y[mask]
    
    # Train the model
    self.model.fit(X_clean, y_clean)
    self.is_fitted = True
```

**Why clean data?** 
- Machine learning models can't handle missing values
- Removing bad data is better than guessing
- Quality of training data determines model performance

**Model evaluation:**
```python
def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
    predictions = self.predict(X)
    
    return {
        'mae': mean_absolute_error(y, predictions),      # Average error
        'rmse': np.sqrt(mean_squared_error(y, predictions)),  # Penalizes large errors
        'r2': r2_score(y, predictions)                  # How well model explains variance
    }
```

**Understanding metrics:**
- **MAE (Mean Absolute Error):** Average difference between prediction and actual
- **RMSE (Root Mean Square Error):** Penalizes large errors more heavily
- **R¬≤ (R-squared):** Percentage of variance explained by the model (0-1, higher is better)

**Try it yourself:**
```python
# Simple example
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np

# Create simple data
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])  # Perfect linear relationship

# Train model
model = LinearRegression()
model.fit(X, y)

# Make prediction
prediction = model.predict([[6]])
print(f"Prediction for 6: {prediction[0]}")  # Should be 12

# Evaluate
predictions = model.predict(X)
mae = mean_absolute_error(y, predictions)
r2 = r2_score(y, predictions)
print(f"MAE: {mae}, R¬≤: {r2}")
```

### Specific Models (`src/models/prediction_models.py`)

**What it does:** Implements different machine learning algorithms, each with strengths and weaknesses.

**Model types and when to use them:**

1. **Random Forest** (`RandomForestModel`)
   - **What it is:** Collection of decision trees, each trained on different data
   - **When to use:** Good baseline, handles non-linear relationships
   - **Pros:** Interpretable, handles missing values
   - **Cons:** Can overfit, slower than linear models

2. **XGBoost** (`XGBoostModel`)
   - **What it is:** Gradient boosting with regularization
   - **When to use:** High performance, complex patterns
   - **Pros:** Very accurate, handles many feature types
   - **Cons:** Can overfit, harder to interpret

3. **Linear Regression** (`LinearRegressionModel`)
   - **What it is:** Straight line through data points
   - **When to use:** Simple relationships, interpretability
   - **Pros:** Fast, interpretable, no overfitting
   - **Cons:** Can't capture complex patterns

**Try different models:**
```python
# Compare models on same data
from src.models.prediction_models import create_model

# Create different model types
rf_model = create_model('random_forest', 'likes')
xgb_model = create_model('xgboost', 'likes')
linear_model = create_model('linear_regression', 'likes')

# Train and compare
models = [rf_model, xgb_model, linear_model]
for model in models:
    model.fit(X_train, y_train)
    metrics = model.evaluate(X_test, y_test)
    print(f"{model.model_name}: MAE={metrics['mae']:.2f}, R¬≤={metrics['r2']:.2f}")
```

---

## üåê Step 4: API Layer - Making Predictions Available

### What We're Learning
- How to create web APIs
- Request/response handling
- Data validation
- Error handling

### The Prediction API (`src/api/prediction_api.py`)

**What it does:** Creates a web service that accepts requests and returns predictions, making your machine learning models accessible to other applications.

**Key Concepts:**
- **FastAPI:** Modern Python web framework
- **Pydantic Models:** Automatic data validation
- **REST Endpoints:** Standard HTTP methods (GET, POST)
- **CORS:** Allows web browsers to call your API

**API structure:**
```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class PredictionRequest(BaseModel):
    platform: str
    content_type: str
    date: str

@app.post("/predict")
async def predict_engagement(request: PredictionRequest):
    # Validate input
    # Load model
    # Make prediction
    # Return result
```

**Why Pydantic?**
- Automatically validates incoming data
- Converts JSON to Python objects
- Provides clear error messages for invalid data

**Try the API:**
```bash
# Start the API
uvicorn src.api.prediction_api:app --reload

# In another terminal, test it
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"platform": "linkedin", "content_type": "no_video", "date": "2023-12-01"}'
```

**What happens when you call the API:**
1. **Request comes in:** JSON data arrives at `/predict` endpoint
2. **Validation:** Pydantic checks if data matches `PredictionRequest` model
3. **Processing:** API loads appropriate model and features
4. **Prediction:** Model makes prediction based on input
5. **Response:** JSON with prediction and confidence returned

---

## üìä Step 5: Dashboard - Visualizing Results

### What We're Learning
- How to create interactive web applications
- Data visualization techniques
- Real-time data integration
- User interface design

### The Dash App (`dash_app/app.py`)

**What it does:** Creates an interactive web dashboard where users can input data and see predictions, visualizations, and insights.

**Key Concepts:**
- **Dash:** Python framework for building web apps
- **Plotly:** Interactive charts and graphs
- **Bootstrap:** Responsive design and styling
- **Callbacks:** Dynamic updates based on user input

**Dashboard structure:**
```python
app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

app.layout = dbc.Container([
    dbc.Tabs([
        dbc.Tab(label="Performance Prediction", children=[...]),
        dbc.Tab(label="Caption Analyzer", children=[...]),
        dbc.Tab(label="Platform Trends", children=[...])
    ])
])
```

**Why tabs?**
- Organizes different types of analysis
- Keeps interface clean and focused
- Allows users to focus on specific tasks

**Interactive components:**
```python
dbc.Dropdown(
    id='platform-dropdown',
    options=[
        {'label': 'LinkedIn', 'value': 'linkedin'},
        {'label': 'Instagram', 'value': 'instagram'}
    ],
    value='linkedin'
)
```

**Callbacks (the magic behind interactivity):**
```python
@app.callback(
    Output('prediction-output', 'children'),
    Input('predict-button', 'n_clicks'),
    State('platform-dropdown', 'value')
)
def update_prediction(n_clicks, platform):
    if n_clicks:
        # Make API call to get prediction
        # Return formatted result
        return f"Predicted engagement for {platform}: ..."
```

**Try the dashboard:**
```bash
# Start dashboard
python dash_app/app.py

# Open browser to http://localhost:8050
```

---

## üöÄ Step 6: Running the Complete Pipeline

### What We're Learning
- How all components work together
- End-to-end data flow
- Testing and validation
- Performance optimization

### The Complete Workflow

**1. Data Collection**
```bash
# Load data from Supabase
python -c "
from src.data.database import SupabaseClient
client = SupabaseClient()
posts = client.get_posts_data()
print(f'Loaded {len(posts)} posts')
"
```

**2. Feature Engineering**
```bash
# Create features
python -c "
from src.features.feature_engineering import FeatureEngineer
from src.data.database import SupabaseClient

# Load data
client = SupabaseClient()
posts = client.get_posts_data()

# Create features
fe = FeatureEngineer()
posts_with_features = fe.create_temporal_features(posts)
posts_with_features = fe.create_lag_features(posts_with_features, ['num_likes_linkedin_no_video'])

print(f'Created {len(posts_with_features.columns)} features')
"
```

**3. Model Training**
```bash
# Train a model
python -c "
from src.models.prediction_models import create_model
from src.features.feature_engineering import FeatureEngineer
from src.data.database import SupabaseClient

# Load and prepare data
client = SupabaseClient()
posts = client.get_posts_data()
fe = FeatureEngineer()
posts_with_features = fe.create_temporal_features(posts)
posts_with_features = fe.create_lag_features(posts_with_features, ['num_likes_linkedin_no_video'])

# Train model
model = create_model('random_forest', 'num_likes_linkedin_no_video')
model.fit(posts_with_features, posts_with_features['num_likes_linkedin_no_video'])

print('Model trained successfully!')
"
```

**4. Make Predictions**
```bash
# Start API and make prediction
uvicorn src.api.prediction_api:app --reload &
sleep 5

curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"platform": "linkedin", "content_type": "no_video", "date": "2023-12-01"}'
```

---

## üîç Step 7: Understanding Model Performance

### What We're Learning
- How to interpret model results
- Feature importance analysis
- Model comparison techniques
- Overfitting detection

### Evaluating Your Models

**1. Check Basic Metrics**
```python
# Load trained model and evaluate
from src.models.base_model import BaseModel
model = BaseModel.load_model('models/random_forest_likes.joblib')

# Get feature importance
importance = model.get_feature_importance()
for feature, score in sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]:
    print(f"{feature}: {score:.3f}")
```

**2. Understand Feature Importance**
- **High importance:** Feature strongly influences predictions
- **Low importance:** Feature has little impact
- **Unexpected importance:** May indicate data leakage or bugs

**3. Detect Overfitting**
- **Training R¬≤ much higher than test R¬≤:** Model memorized training data
- **Solution:** Use simpler model, more regularization, or more training data

**4. Compare Models**
```python
# Compare multiple models
models = ['random_forest', 'xgboost', 'linear_regression']
results = {}

for model_type in models:
    model = create_model(model_type, 'num_likes_linkedin_no_video')
    model.fit(X_train, y_train)
    metrics = model.evaluate(X_test, y_test)
    results[model_type] = metrics

# Find best model
best_model = min(results.keys(), key=lambda x: results[x]['mae'])
print(f"Best model: {best_model} with MAE: {results[best_model]['mae']:.3f}")
```

---

## üõ†Ô∏è Step 8: Customization and Extension

### What We're Learning
- How to modify existing functionality
- Adding new features
- Integrating external data sources
- Scaling the system

### Adding New Features

**1. Create a New Feature Type**
```python
# In src/features/feature_engineering.py
def create_holiday_features(self, df: pd.DataFrame, date_col: str = 'date') -> pd.DataFrame:
    """Create holiday-specific features."""
    df = df.copy()
    
    # Add holiday indicators
    df['is_christmas'] = ((df[date_col].dt.month == 12) & 
                          (df[date_col].dt.day >= 20) & 
                          (df[date_col].dt.day <= 26)).astype(int)
    
    df['is_new_year'] = ((df[date_col].dt.month == 1) & 
                         (df[date_col].dt.day <= 3)).astype(int)
    
    return df
```

**2. Add New Model Type**
```python
# In src/models/prediction_models.py
class NeuralNetworkModel(BaseModel):
    def _create_model(self):
        from sklearn.neural_network import MLPRegressor
        return MLPRegressor(
            hidden_layer_sizes=(100, 50),
            max_iter=1000,
            random_state=42
        )

# Add to factory function
def create_model(model_type: str, target_column: str, **kwargs) -> BaseModel:
    if model_type == 'neural_network':
        return NeuralNetworkModel(target_column, **kwargs)
    # ... existing code ...
```

**3. Create New API Endpoint**
```python
# In src/api/prediction_api.py
@app.post("/analyze-trends")
async def analyze_trends(request: TrendRequest):
    """Analyze posting trends and patterns."""
    # Implementation here
    pass
```

---

## üìà Step 9: Advanced Concepts

### What We're Learning
- Hyperparameter tuning
- Cross-validation
- Ensemble methods
- Model deployment

### Hyperparameter Tuning

**What are hyperparameters?** Settings that control how models learn (e.g., number of trees in Random Forest, learning rate in XGBoost).

**Why tune them?** Default values are rarely optimal for your specific data.

**Using Optuna for tuning:**
```python
import optuna

def objective(trial):
    # Define hyperparameter search space
    n_estimators = trial.suggest_int('n_estimators', 50, 300)
    max_depth = trial.suggest_int('max_depth', 3, 10)
    
    # Create and train model
    model = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=42
    )
    
    # Evaluate
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    return mean_absolute_error(y_test, predictions)

# Run optimization
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

print(f"Best parameters: {study.best_params}")
```

### Cross-Validation

**What is it?** Testing model performance on multiple subsets of data to get more reliable estimates.

**Why use it?** Single train/test split can be misleading due to random variation.

```python
from sklearn.model_selection import cross_val_score

# 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')
mae_scores = -scores  # Convert to positive MAE

print(f"Cross-validation MAE: {mae_scores.mean():.3f} (+/- {mae_scores.std() * 2:.3f})")
```

---

## üéØ Step 10: Production Deployment

### What We're Learning
- Containerization with Docker
- Environment management
- Monitoring and logging
- Scaling considerations

### Docker Deployment

**Why Docker?**
- Consistent environment across machines
- Easy deployment and scaling
- Isolated dependencies

**Building and running:**
```bash
# Build image
docker build -t content-predictor .

# Run container
docker run -p 8000:8000 -p 8050:8050 content-predictor
```

**Docker Compose for multiple services:**
```yaml
# docker-compose.yml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
  
  dashboard:
    build: .
    command: python dash_app/app.py
    ports:
      - "8050:8050"
```

---

## üîÑ Step 11: Continuous Learning and Improvement

### What We're Learning
- Model retraining strategies
- A/B testing
- Performance monitoring
- Feedback loops

### Model Retraining

**When to retrain?**
- New data available
- Performance degradation
- Business requirements change

**Automated retraining:**
```python
# Check model performance daily
def check_model_performance():
    current_metrics = evaluate_current_model()
    if current_metrics['mae'] > threshold:
        retrain_model()
        log_model_update()
```

---

## üìù Final Step: Creating Your LLM Prompt

Now that you understand the complete system, here's a prompt you can use with any LLM to recreate this learning experience:

---

## ü§ñ LLM Prompt for Data Science Project Learning

```
I want to learn data science by building a complete project from scratch. I have technical knowledge and programming experience, but I'm new to machine learning. I want you to:

1. **Analyze the project structure** I provide and explain why it's organized this way
2. **Break down each component** step-by-step, explaining what it does and why it exists
3. **Provide hands-on examples** for each concept, not just code explanations
4. **Explain the data science concepts** behind each tool and technique
5. **Create a learning path** that builds understanding progressively
6. **Include practical exercises** I can run to see concepts in action
7. **Explain the "why"** behind design decisions, not just the "how"
8. **Cover the complete pipeline** from data collection to model deployment
9. **Include troubleshooting** and common pitfalls to avoid
10. **Provide a final prompt** I can reuse for other projects

For each component, explain:
- What it does and why it's needed
- How it fits into the overall system
- The data science concepts it implements
- How to test and validate it works
- Common issues and how to debug them

Make this a comprehensive learning experience where I understand the complete system, not just how to run it. Include code examples I can execute to see concepts in action.

Project details: [Describe your specific project here]
```

---

## üéâ Congratulations!

You've now completed a comprehensive journey through data science concepts by building and understanding a real project. You've learned:

- **Data Engineering:** How to collect, clean, and structure data
- **Feature Engineering:** How to transform raw data into useful features
- **Machine Learning:** How different algorithms work and when to use them
- **Model Evaluation:** How to assess and improve model performance
- **API Development:** How to make models accessible to other applications
- **Dashboard Creation:** How to visualize results and insights
- **Deployment:** How to put your system into production

**Next Steps:**
1. **Experiment:** Try different features, models, and parameters
2. **Extend:** Add new data sources or prediction targets
3. **Optimize:** Improve performance and accuracy
4. **Apply:** Use these concepts in your own projects

**Remember:** Data science is iterative. Start simple, measure results, and gradually improve. The key is understanding what each piece does and why it matters.

Happy learning! üöÄ

---

## üìö Additional Resources

### Books
- "Hands-On Machine Learning" by Aur√©lien G√©ron
- "Python for Data Analysis" by Wes McKinney
- "The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman

### Online Courses
- Coursera: Machine Learning by Andrew Ng
- edX: Data Science MicroMasters
- Fast.ai: Practical Deep Learning for Coders

### Communities
- Kaggle: Data science competitions and datasets
- Stack Overflow: Technical questions and answers
- Reddit: r/MachineLearning, r/datascience

### Tools to Explore
- Jupyter Notebooks: Interactive development
- Streamlit: Alternative to Dash for quick prototypes
- MLflow: Experiment tracking and model management
- Optuna: Hyperparameter optimization
- Weights & Biases: Alternative to MLflow

---

## üîß Troubleshooting Common Issues

### Database Connection Issues
- **Problem:** "SUPABASE_URL not found"
- **Solution:** Check your `.env` file and ensure variables are set
- **Debug:** Print `os.getenv("SUPABASE_URL")` to verify

### Model Training Issues
- **Problem:** "No valid data after removing missing values"
- **Solution:** Check your data for NaN values and handle them appropriately
- **Debug:** Use `df.isnull().sum()` to see missing value counts

### API Issues
- **Problem:** "Model not fitted"
- **Solution:** Ensure you've trained the model before making predictions
- **Debug:** Check `model.is_fitted` attribute

### Dashboard Issues
- **Problem:** Callbacks not working
- **Solution:** Verify input/output IDs match between layout and callback
- **Debug:** Check browser console for JavaScript errors

---

## üéØ Final Exercise: Build Your Own Feature

Now that you understand the system, try building something new:

1. **Create a new feature** that predicts engagement based on weather data
2. **Add a new model type** (try a different algorithm)
3. **Create a new API endpoint** for trend analysis
4. **Add a new dashboard tab** for your feature

This will solidify your understanding and show you how to extend the system.

**Remember:** The best way to learn is by doing. Don't just read this guide - implement each step, experiment with the code, and build your understanding piece by piece.

Good luck on your data science journey! üöÄ